# üß† 100 AI Engineer Notebooks for Healthcare :
- Complete learning program in Artificial Intelligence applied to healthcare, from linear regression to transformers, using PyTorch.
-  See PDF 100_notebook_AI_detail for details about data used and more

## üìñ About the Project

This repository contains 100 progressive Jupyter notebooks covering all the skills required to become an AI engineer in healthcare. Each notebook is designed to be **content-rich**, **practical**, and **directly applicable** to real-world problems.

## üìö Program Structure:

### Block 1: Python & Data Science Foundations (1-15)
- NumPy, Pandas, Matplotlib, Seaborn
- Web scraping (BeautifulSoup, Selenium)
- Feature engineering and ML pipelines
- Hyperparameter optimization

### Block 2: Classic Machine Learning (16-30)
- Regression (Linear, Ridge, Lasso, Logistic)
- SVM, Decision Trees, Random Forest
- Gradient Boosting (XGBoost, LightGBM, CatBoost)
- Clustering (K-Means, DBSCAN, GMM)
- Dimensionality reduction (PCA, t-SNE)

### Block 3: Deep Learning with PyTorch (31-45)
- PyTorch fundamentals (tensors, autograd)
- Neural network architectures
- Optimizers, regularization, normalization
- Transfer learning and fine-tuning
- Mixed precision training

### Block 4: Medical Computer Vision (46-60)
- Classic CNNs (AlexNet, VGG, ResNet, EfficientNet)
- Vision Transformers (ViT)
- Segmentation (U-Net, nnU-Net)
- Object Detection (YOLO, Faster R-CNN)
- GANs for medical image generation
- 3D Imaging (CT scans, MRI)

### Block 5: NLP for Medical Data (61-72)
- Clinical text preprocessing
- Word embeddings, LSTM, Transformers
- BERT, BioBERT, ClinicalBERT
- Medical NER, relation extraction
- Question Answering, clinical summaries
- Medical chatbot

### Block 6: Medical Time Series (73-80)
- ARIMA, LSTM for time series
- TCN, WaveNet
- Transformers for time series
- Anomaly detection (ECG, EEG)
- Critical event prediction

### Block 7: Interpretability & Explainability (81-88)
- SHAP, LIME
- Grad-CAM, Integrated Gradients
- Attention visualization
- Counterfactual explanations
- Explainability dashboard

### Block 8: Advanced Topics & Production (89-100)
- Federated Learning
- Differential Privacy
- Quantization, ONNX
- Docker, MLflow, FastAPI
- CI/CD for ML
- Production monitoring
- Active Learning, AutoML

## üõ†Ô∏è Technologies Used

**Languages & Frameworks:**
- Python 3.9+
- PyTorch (main framework)
- Scikit-learn
- Pandas, NumPy, Matplotlib, Seaborn

**Specialized Libraries:**
- Hugging Face Transformers
- Albumentations (augmentation)
- XGBoost, LightGBM, CatBoost
- SHAP, LIME (explainability)
- FastAPI (deployment)
- MLflow (tracking)

**Tools:**
- Jupyter Notebook / JupyterLab
- Git & GitHub
- Docker
- Optuna (hyperparameter tuning)

## üìä Datasets Used

The program covers over 50 different datasets, including:

**Public medical datasets:**
- MIMIC-III (clinical notes, signals)
- NIH Chest X-ray14
- ISIC Skin Lesion
- PhysioNet (ECG, EEG)
- Diabetic Retinopathy Detection
- PubMed articles

**Generic datasets:**
- MNIST, Fashion-MNIST, CIFAR-10
- ImageNet (subset)
- IMDB Reviews

**Sources:**
- UCI Machine Learning Repository
- Kaggle
- PhysioNet
- OpenML

## üìö Additional Resources

**Recommended books:**
- "Deep Learning" - Ian Goodfellow
- "Hands-On Machine Learning" - Aur√©lien G√©ron
- "Deep Learning with PyTorch" - Eli Stevens
- "Pattern Recognition and Machine Learning" - Christopher Bishop

**Communities:**
- PyTorch Forums
- Kaggle Discussions
- Reddit r/MachineLearning
- Papers with Code

## ü§ù Contribution

Contributions are welcome! If you would like to:
- Fix errors
- Improve code
- Add explanations
- Propose new notebooks
